# Aparavi Data Suite MCP Server

A comprehensive Model Context Protocol (MCP) server that enables Claude Desktop and other MCP clients to interact with Aparavi Data Suite systems through natural language. The server provides a unified interface for executing AQL (Aparavi Data Suite Query Language) queries, running predefined reports, and validating custom queries.

## ⚡ Quick Start

**🌟 New to the Aparavi Data Suite MCP Server?**

1. **First Time Users**: Simply call `guide_start_here` with no parameters for personalized guidance
2. **Experienced Users**: Use `guide_start_here` with your specific goals for optimized workflows
3. **Need Help?**: The intelligent entry point adapts to your experience level automatically

```json
{
  "name": "guide_start_here",
  "arguments": {}
}
```

**That's it!** The system will assess your needs and guide you to the perfect tool sequence.

## 🚀 Features

### Core MCP Tools
- **`guide_start_here`**: **🌟 START HERE** - Intelligent entry point and routing assistant that assesses your experience, goals, and preferences to provide personalized guidance on which tools to use and in what sequence
- **`health_check`**: Comprehensive server health monitoring with API connectivity and AQL validation
- **`server_info`**: Detailed server configuration and capability information
- **`run_aparavi_report`**: Execute 20 predefined Aparavi Data Suite reports and 5 analysis workflows
- **`validate_aql_query`**: Validate custom AQL queries without execution
- **`execute_custom_aql_query`**: Validate and execute custom AQL queries with raw JSON output
- **`generate_aql_query`**: Intelligent AQL query builder for custom analysis not covered by predefined reports
- **`manage_tag_definitions`**: Create, list, or delete tag definitions in the global tag registry
- **`apply_file_tags`**: Apply or remove tags from files using bulk operations with direct file specification or AQL query-based selection
- **`search_files_by_tags`**: Search files using tag-based criteria with advanced filtering options and include/exclude logic
- **`tag_workflow_operations`**: Execute high-level tagging workflows for common use cases like bulk tagging, retagging, and cleanup

### 🎯 Tool Selection Guide

**🌟 NEW USERS OR UNSURE WHERE TO START?**

**Use `guide_start_here` first!** This intelligent assistant will assess your needs and recommend the perfect tool sequence.

**When to Use Each Tool:**

1. **`guide_start_here`** - **START HERE** for personalized guidance and tool recommendations
   - Perfect for: New users, complex analysis planning, when unsure which tool to use
   - Adapts to your experience level and analysis goals
   - Provides step-by-step workflows tailored to your needs

2. **`run_aparavi_report`** - Use for standard business questions covered by the 20 predefined reports
   - Example: "Show me duplicate files" or "Analyze storage by data source"
   - Use `report_name="list"` to see all available reports

3. **`generate_aql_query`** - Use for custom analysis NOT covered by predefined reports
   - Example: "Find Excel files with macros created in Q3 2024 by department"
   - Prevents syntax errors and generates valid AQL from natural language
   - **Always use this first** for custom queries before validation/execution

4. **`validate_aql_query`** - Use to check AQL syntax before execution
   - Validates queries generated by `generate_aql_query` or written manually
   - Returns detailed syntax error information if invalid

5. **`execute_custom_aql_query`** - Use to run validated custom AQL queries
   - Combines validation + execution in one step
   - Returns raw JSON results for analysis

6. **`health_check`** - Use to verify system status and API connectivity
7. **`server_info`** - Use to view server configuration and capabilities

8. **`manage_tag_definitions`** - Use to manage the global tag registry
   - Create new tags for classification and organization
   - List all available tags to see what's already defined
   - Delete unused tags to keep the registry clean

9. **`apply_file_tags`** - Use to tag files in bulk operations
   - Tag files directly by specifying file objects (objectId + instanceId)
   - Tag files found by AQL queries for efficient bulk operations
   - Remove tags from files when reorganizing or cleaning up

10. **`search_files_by_tags`** - Use to find files based on their tags
    - Search files that have specific tags (include logic)
    - Exclude files with certain tags (exclude logic)
    - Combine tag searches with additional AQL filters
    - Use AND/OR logic for complex tag-based queries

11. **`tag_workflow_operations`** - Use for complex tagging workflows
    - Execute common tagging patterns like "find and tag"
    - Perform bulk retagging operations
    - Generate tag reports and analytics
    - Clean up and reorganize tag structures

**Recommended Workflows:**
```
🆕 New User: guide_start_here → [recommended tool sequence]
📊 Standard Analysis: run_aparavi_report
🔧 Custom Analysis: generate_aql_query → validate_aql_query → execute_custom_aql_query
🏷️ File Tagging: manage_tag_definitions → apply_file_tags → search_files_by_tags
📋 Tag Management: manage_tag_definitions → tag_workflow_operations
🔍 Tag-based Search: search_files_by_tags → [analysis or further tagging]
🚨 Troubleshooting: health_check → [diagnostic steps]
```

### 🌟 Guide Start Here - Usage Examples

The `guide_start_here` tool is your intelligent entry point that adapts to your needs:

**🆕 Complete Beginner (No Parameters Needed):**
```json
{
  "name": "guide_start_here",
  "arguments": {}
}
```
*Result: Gentle introduction with safe defaults and immediate actionable steps*

**📊 Specific Analysis Goal:**
```json
{
  "name": "guide_start_here",
  "arguments": {
    "user_experience": "new",
    "query_goal": "duplicates",
    "context_window": "small"
  }
}
```
*Result: Focused guidance to duplicate analysis reports with minimal complexity*

**🔧 Custom Analysis Planning:**
```json
{
  "name": "guide_start_here",
  "arguments": {
    "user_experience": "intermediate",
    "query_goal": "custom",
    "specific_question": "Find Excel files with macros created by specific users in Q3 2024",
    "context_window": "large"
  }
}
```
*Result: Comprehensive workflow from query generation through execution with detailed explanations*

**🚨 Troubleshooting Assistance:**
```json
{
  "name": "guide_start_here",
  "arguments": {
    "query_goal": "troubleshooting",
    "specific_question": "My AQL queries keep failing with syntax errors"
  }
}
```
*Result: Diagnostic workflow starting with health_check and validation guidance*

**⚡ Advanced User Quick Reference:**
```json
{
  "name": "guide_start_here",
  "arguments": {
    "user_experience": "advanced",
    "preferred_approach": "custom_queries",
    "context_window": "small"
  }
}
```
*Result: Direct routing to validation and execution tools with minimal explanation*

### Advanced Capabilities
- **🧠 Intelligent Entry Point**: `guide_start_here` provides personalized guidance based on user experience and goals
- **🎯 Adaptive Workflows**: Context-aware responses that scale from focused steps to comprehensive guidance
- **📊 20 Predefined Reports**: Comprehensive data analysis covering storage, duplicates, classifications, and more
- **🔗 5 Analysis Workflows**: Pre-configured multi-report analysis for common business scenarios
- **🤖 Intelligent AQL Generation**: Natural language to AQL conversion with syntax validation
- **✅ Comprehensive Field Reference**: Built-in validation against available Aparavi fields
- **🛡️ Error Prevention**: Avoids common AQL pitfalls (DISTINCT, DATEADD, invalid fields)
- **📋 Raw JSON Output**: Direct API results for further processing and visualization
- **🔍 Health Monitoring**: Continuous validation of all configured queries
- **🎨 Professional Integration**: Seamless Claude Desktop MCP integration
- **📱 Multi-Model Support**: Optimized for both small and large language models
- **🎛️ Experience-Adaptive**: Automatically adjusts complexity based on user skill level with environment-based credentials
- **Robust Error Handling**: Comprehensive error reporting and recovery mechanisms
- **Modern Architecture**: Python 3.11+ with async/await and UV package management

## 📋 Prerequisites

- **Python 3.11+**: Required for modern async features
- **UV Package Manager**: Fast dependency management
- **Aparavi Data Suite System**: Access to Aparavi Data Suite API
- **Claude Desktop**: For MCP integration (or any MCP-compatible client)

## 🛠️ Installation

### 1. Install UV Package Manager

**Windows (PowerShell):**
```powershell
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```

**macOS/Linux:**
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 2. Clone and Setup Project

```bash
# Clone the repository
git clone <repository-url>
cd aparavi_reporting_mcp

# Create virtual environment and install dependencies
uv venv
uv pip install -e .

# Install development dependencies (optional)
uv pip install -e ".[dev]"
```

### 3. Environment Configuration

Copy the environment template and configure your Aparavi Data Suite connection:

```bash
cp .env.template .env
```

Edit `.env` with your Aparavi Data Suite credentials:

```env
# Aparavi Data Suite API Configuration
APARAVI_BASE_URL=http://localhost
APARAVI_PORT=80
APARAVI_USERNAME=your_username
APARAVI_PASSWORD=your_password

# Server Configuration
SERVER_NAME=Aparavi Data Suite MCP Server
SERVER_VERSION=1.0.0
LOG_LEVEL=INFO
```

### 4. Claude Desktop Integration

Add the MCP server to your Claude Desktop configuration:

**Location:** `%APPDATA%\Claude\claude_desktop_config.json` (Windows) or `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS)

```json
{
  "mcpServers": {
    "aparavi-reporting": {
      "command": "python",
      "args": [
        "C:\\path\\to\\aparavi_reporting_mcp\\scripts\\start_server_claude.py"
      ],
      "cwd": "C:\\path\\to\\aparavi_reporting_mcp"
    }
  }
}
```

## 📖 Usage

### Starting the Server

**For Claude Desktop (recommended):**
The server starts automatically when Claude Desktop loads. Check the MCP section for available tools.

**For standalone testing:**
```bash
# Activate virtual environment
source .venv/bin/activate  # Linux/macOS
# or
.venv\Scripts\activate     # Windows

# Start server
python scripts/start_server.py
```

### 🔧 Available MCP Tools

#### 1. `guide_start_here`
**Purpose:** 🌟 **START HERE** - Intelligent entry point and routing assistant  
**Parameters:**
- `user_experience` (optional): "new", "intermediate", "advanced", or "unknown"
- `query_goal` (optional): "duplicates", "growth", "security", "exploration", "custom", "troubleshooting", or "unknown"
- `preferred_approach` (optional): "reports", "custom_queries", "guided", or "unknown"
- `context_window` (optional): "small", "medium", "large", or "unknown"
- `specific_question` (optional): Describe your specific data analysis question or challenge

**Features:**
- Assesses your experience level and analysis goals
- Provides personalized step-by-step workflows
- Adapts response complexity to your preferences
- Routes you to the optimal tool sequence
- Perfect for new users or complex analysis planning

#### 2. `health_check`
**Purpose:** Comprehensive server health monitoring  
**Parameters:** None  
**Features:**
- API connectivity testing
- AQL query validation for all configured reports
- Configuration validation
- Detailed status reporting

#### 3. `server_info`
**Purpose:** Detailed server configuration and capabilities  
**Parameters:** None  
**Returns:** Server version, configuration, loaded reports count

#### 4. `run_aparavi_report`
**Purpose:** Execute predefined reports and analysis workflows  
**Parameters:**
- `report_name` (optional): Specific report name or "list"
- `workflow_name` (optional): Workflow name or "list"

#### 5. `validate_aql_query`
**Purpose:** Validate custom AQL queries without execution  
**Parameters:**
- `query` (required): AQL query string to validate
**Returns:** Validation status, syntax errors, recommendations

#### 6. `execute_custom_aql_query`
**Purpose:** Validate and execute custom AQL queries  
**Parameters:**
- `query` (required): AQL query string to validate and execute
**Returns:** Raw JSON results for LLM interpretation

#### 7. `generate_aql_query`
**Purpose:** Intelligent AQL query builder for custom analysis  
**Parameters:**
- `business_question` (required): The specific business question or data analysis need
- `desired_fields` (optional): Specific fields/columns desired in the output
- `filters` (optional): Specific filter conditions
- `complexity_preference` (optional): "simple" or "comprehensive" approach
**Features:**
- Generates valid AQL syntax from natural language
- Prevents common syntax errors (DISTINCT, DATEADD, invalid fields)
- Provides detailed explanations and field validation
- Recommends query chaining for complex analysis

### 📊 Available Reports (20 Total)

**Storage & Optimization:**
- `data_sources_overview` - Storage overview by data source
- `subfolder_overview` - Storage breakdown by subfolder
- `file_type_analysis` - File type distribution and sizes
- `large_files_analysis` - Files over 100MB
- `storage_optimization` - Storage optimization opportunities
- `archive_candidates` - Files suitable for archiving
- `cleanup_opportunities` - Data cleanup recommendations

**Data Quality & Duplicates:**
- `duplicate_files_analysis` - Duplicate file detection
- `version_control_analysis` - File versioning patterns

**Security & Compliance:**
- `classification_overview` - Data classification summary
- `sensitive_data_analysis` - Sensitive data identification
- `retention_analysis` - Data retention compliance
- `compliance_summary` - Compliance status overview
- `risk_assessment` - Data risk evaluation

**Analytics & Insights:**
- `data_growth_analysis` - Historical data growth trends
- `user_activity_analysis` - User access patterns
- `metadata_analysis` - File metadata insights
- `access_patterns` - File access analytics
- `collaboration_insights` - File sharing analytics
- `performance_metrics` - System performance data

### 🔄 Analysis Workflows (5 Total)

- **`storage_optimization`** - Complete storage analysis and recommendations
- **`data_governance`** - Governance and compliance review
- **`security_assessment`** - Security-focused analysis
- **`cleanup_analysis`** - Data cleanup recommendations
- **`performance_review`** - Performance optimization insights

### 💬 Example Queries

**Natural Language Examples:**
```sql
"Show me the health status of the server"
"Run a storage optimization analysis"
"What are the largest files in the system?"
"Show me duplicate files that could be cleaned up"
"Run the data governance workflow"
"List all available reports"
"Validate this AQL query: SELECT name WHERE ClassID = 'idxobject' LIMIT 5"
"Execute this query and show results: SELECT extension, COUNT(*) WHERE ClassID = 'idxobject' GROUP BY extension"
```

## 🧪 Development & Testing

### Project Structure

```bash
aparavi_reporting_mcp/
├── src/aparavi_mcp/           # Main source code
│   ├── server.py              # MCP server implementation
│   ├── aparavi_client.py      # Aparavi Data Suite API client
│   ├── config.py              # Configuration management
│   └── utils.py               # Utility functions
├── config/                    # Configuration files
│   └── aparavi_reports.json   # Report and workflow definitions
├── scripts/                   # Utility scripts
│   ├── start_server.py        # Standalone server starter
│   ├── start_server_claude.py # Claude Desktop wrapper
│   ├── test_mcp_manual.py     # Manual MCP testing
│   ├── test_aql_validation.py # AQL validation testing
│   └── test_execute_custom_aql.py # Custom query testing
├── references/                # Documentation
│   └── aparavi_aql_reports.md # Comprehensive AQL reference
└── tests/                     # Test suite
```

### Running Tests

```bash
# Manual MCP server testing
python scripts/test_mcp_manual.py

# Test AQL validation functionality
python scripts/test_aql_validation.py

# Test custom query execution
python scripts/test_execute_custom_aql.py

# Simple validation test
python scripts/simple_mcp_test.py
```

## 🔧 Troubleshooting

### Common Issues

1. **Server Won't Start**
   - Check Python version (3.11+ required)
   - Verify virtual environment activation: `uv venv` then activate
   - Check `.env` file exists and has correct values

2. **Authentication Failed**
   - Verify Aparavi Data Suite credentials in `.env`
   - Test connectivity: `curl -u username:password http://localhost:80/server/api/v3/database/query`
   - Ensure Aparavi Data Suite server is running and accessible

3. **Claude Desktop Integration Issues**
   - Verify `claude_desktop_config.json` syntax is valid JSON
   - Use absolute paths in configuration
   - Restart Claude Desktop after config changes
   - Check Windows path escaping: `C:\\path\\to\\file`

4. **Query Execution Errors**
   - Use `validate_aql_query` tool to check syntax first
   - Remember: always include `ClassID = 'idxobject'` for file queries
   - No `COUNT(DISTINCT)` - use proper AQL syntax
   - Check Aparavi Data Suite server logs for detailed errors

5. **Unicode/Encoding Errors**
   - This project avoids Unicode emojis for Windows compatibility
   - If you see encoding errors, check for Unicode characters in logs

### Debug Mode

**Enable Debug Logging:**
```env
LOG_LEVEL=DEBUG
```

**Test Health Check:**
```bash
python -c "import asyncio; from src.aparavi_mcp.server import AparaviMCPServer; asyncio.run(AparaviMCPServer()._handle_health_check())"
```

## 📚 AQL Reference

### Core AQL Syntax Rules

The server uses Aparavi Data Suite Query Language (AQL) for data queries. Key syntax rules:

- **Always include:** `ClassID = 'idxobject'` for file queries
- **Field names:** `name`, `path`, `parentPath`, `size`, `extension`, `createTime`, `modifyTime`
- **Metadata filtering:** `metadata LIKE '%"Field":"Value"%'`
- **Size units:** 1073741824 = 1GB, 1048576 = 1MB
- **No SQL functions:** Avoid `COUNT(DISTINCT)`, use AQL-specific syntax
- **Classification:** `classification != 'Unclassified'` to exclude unclassified

### Example AQL Queries

**Basic file listing:**
```sql
SELECT name, size, extension 
WHERE ClassID = 'idxobject' 
LIMIT 10
```

**Large files analysis:**
```sql
SELECT name, size/1048576 AS "Size (MB)", path
WHERE ClassID = 'idxobject' 
  AND size > 104857600
ORDER BY size DESC
LIMIT 20
```

**File type distribution:**
```sql
SELECT extension, COUNT(*) AS "File Count", SUM(size)/1073741824 AS "Total Size (GB)"
WHERE ClassID = 'idxobject'
GROUP BY extension
ORDER BY "Total Size (GB)" DESC
```

**Metadata search:**
```sql
SELECT name, path
WHERE ClassID = 'idxobject'
  AND metadata LIKE '%"Author":"John Doe"%'
LIMIT 50
```

## 🤝 Contributing

1. **Fork the repository**
2. **Create feature branch:** `git checkout -b feature/amazing-feature`
3. **Follow coding standards:**
   - Python 3.11+ syntax and type hints
   - Async/await for I/O operations
   - No Unicode emojis (Windows compatibility)
   - Comprehensive error handling
4. **Add tests** for new functionality
5. **Test thoroughly** with provided test scripts
6. **Commit changes:** `git commit -m 'Add amazing feature'`
7. **Push and create Pull Request**

## 📄 License

MIT License - see [LICENSE](LICENSE) file for details.

## 🆘 Support

- **Issues:** [GitHub Issues](https://github.com/your-org/aparavi_reporting_mcp/issues)
- **Documentation:** [AQL Reference Guide](references/aparavi_aql_reports.md)
- **Testing:** Use provided test scripts in `scripts/` directory

---

**Built for seamless Aparavi Data Suite management through Claude Desktop** 🚀
